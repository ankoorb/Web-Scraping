{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Scrapers**  \n",
    "Python 3 - Codes: https://github.com/REMitchell/python-scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<urllib2.Request instance at 0x1027af560>\n",
      "\n",
      "\n",
      "<addinfourl at 4336580368 whose fp = <socket._fileobject object at 0x1027155d0>>\n",
      "\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>A Useful Page</title>\n",
      "</head>\n",
      "<body>\n",
      "<h1>An Interesting Title</h1>\n",
      "<div>\n",
      "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# scrapetest.py\n",
    "import urllib2\n",
    "from urllib2 import Request\n",
    "url = 'http://pythonscraping.com/pages/page1.html'\n",
    "request = Request(url)\n",
    "html = urllib2.urlopen(request)\n",
    "print request\n",
    "print '\\n'\n",
    "print html\n",
    "print '\\n'\n",
    "print html.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python documentation for urllib2 library: https://docs.python.org/2/library/urllib.html  \n",
    "\n",
    "Check book for setting up and leaving **environments**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n",
      "<h1>An Interesting Title</h1>\n",
      "<h1>An Interesting Title</h1>\n",
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "# beautifulSoup.py\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'http://pythonscraping.com/pages/page1.html'\n",
    "html = urllib2.urlopen(url)\n",
    "bsObj = BeautifulSoup(html.read()) # .read() gets the HTML content of the page\n",
    "print bsObj.h1\n",
    "print bsObj.html.body.h1\n",
    "print bsObj.body.h1\n",
    "print bsObj.html.h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connecting Reliably** - Handling exceptions\n",
    "```python\n",
    "html = urllib2.urlopen('http://pythonscraping.com/pages/page1.html')\n",
    "```\n",
    "Two main things can go wrong in the above line:\n",
    "- Page is not found on the server: HTTPError\n",
    "```python\n",
    "try:\n",
    "html = urllib2.urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "except HTTPError as e:\n",
    "print e\n",
    "#return null, break, or do some other \"Plan B\"\n",
    "else:\n",
    "#program continues. Note: If you return or break in the\n",
    "#exception catch, you do not need to use the \"else\" statement\n",
    "```\n",
    "- Server is not found (server is down or url mistyped): \n",
    "```python\n",
    "if html is None:\n",
    "print(\"URL is not found\")\n",
    "else:\n",
    "#program continues\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1>An Interesting Title</h1>\n"
     ]
    }
   ],
   "source": [
    "# exceptionHandling.py\n",
    "import urllib2\n",
    "from urllib2 import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "def getTitle(url):\n",
    "    \n",
    "    # Check for HTTPError\n",
    "    try:\n",
    "        html = urllib2.urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    # Check for AttributeError - If server did not exist, html would be a \n",
    "    # None object, and html.read() would throw an AttributeError\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read())\n",
    "        title = bsObj.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    \n",
    "    return title\n",
    "        \n",
    "title = getTitle(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "if title == None:\n",
    "    print 'Title could not be found'\n",
    "else:\n",
    "    print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
